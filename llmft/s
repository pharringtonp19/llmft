    def train(self, data_loader):
        self.model.train()
        total_loss = 0
        total_samples = 0
        total_neg_log_prob = 0
        self.optimizer.zero_grad()

        for i, batch in enumerate(data_loader):
            loss, log_prob_target = self.process_batch(batch)
            neg_log_prob = -log_prob_target.mean().item()

            total_samples += len(batch)
            total_loss += loss.item() * len(batch)
            total_neg_log_prob += neg_log_prob * len(batch)

            loss = loss / self.gradient_accumulation  # Normalizing loss for accumulation steps
            loss.backward()

            if (i + 1) % self.gradient_accumulation == 0:  # Optimizer step after 'accumulation_steps' batches
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()
                if self.verbose:
                    print(f"Batch {i+1} Loss: {loss.item() * self.gradient_accumulation}")
                    print(f"Batch {i+1} Negative Log Probability: {neg_log_prob}")

                torch.cuda.empty_cache()

        average_loss = total_loss / total_samples
        average_neg_log_prob = total_neg_log_prob / total_samples

        return average_loss, average_neg_log_prob