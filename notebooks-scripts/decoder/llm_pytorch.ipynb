{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"Cude is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Other Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import DataCollatorWithPadding\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random \n",
    "from datasets import Dataset, DatasetDict\n",
    "import warnings\n",
    "from functools import partial\n",
    "from datasets import concatenate_datasets\n",
    "from functools import partial \n",
    "from tqdm import tqdm \n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "import os \n",
    "import re \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "warnings.filterwarnings('ignore', message='Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.')\n",
    "from llmft.train import DecoderTrainer, EarlyStopping\n",
    "from llmft.metrics import compute_recall\n",
    "from llmft.losses import FocalLoss\n",
    "from llmft.utils import predict, log_predictions\n",
    "from llmft.generate import generate_dataset\n",
    "from llmft.paper import generate_recall_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with `parameters`\n",
    "\"meta-llama/Meta-Llama-3-8B-Instruct\" #\"google/gemma-1.1-7b-it\" #microsoft/phi-2\" #\"microsoft/phi-2\" #\"#\"meta-llama/Llama-2-7b-chat-hf\" # \"distilbert-base-uncased\" \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "column = 'text'\n",
    "epochs = 1\n",
    "seed = 0\n",
    "verbose = True \n",
    "test_size = 0.5\n",
    "p = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Qlora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# ----- QUANTIZATION -------# \n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# ----- LORA -------# \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instantiate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             trust_remote_code=True)# So we can do gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.gradient_checkpointing = True\n",
    "model.enable_input_require_grads()\n",
    "print(model.generation_config)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenizer_function(example):\n",
    "  return tokenizer.apply_chat_template(example[\"messages\"], \n",
    "                                                          tokenize=True, \n",
    "                                                          add_generation_prompt=False, \n",
    "                                                          return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "warmup_ratio = 0\n",
    "batch_size = 4\n",
    "epochs = 30\n",
    "patience = float('inf') \n",
    "gamma = 0.0\n",
    "training_status = 'standard' if lr==1e-4 else 'preferred'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset =  [[{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: Yes, the Right to Counsel is in effect in the tenant's zip code. The tenant violated the lease.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'No', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: Yes, the Right to Counsel is in effect in the tenant's zip code. The tenant lost their job because of health related accident.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'No', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a smart AI', 'role': 'system'},\n",
    "                    {'content': \"In three words, in Economics, the acronym 'llm' stands for what? \", 'role': 'user'},\n",
    "                    {'content': 'Joe Biden', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a smart AI', 'role': 'assistant'},\n",
    "                    {'content': \"In one word, how good at Basketball is Lebron James?\", 'role': 'user'},\n",
    "                    {'content': 'Excellent', 'role': 'assistant'}],\n",
    "                    ]\n",
    "random_dataset =  Dataset.from_dict({'messages': random_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"ppower1/chat_instrument\")['train']\n",
    "# labels = np.array([1 if i[2]['content'] == 'Yes' else 0 for i in dataset['messages']])\n",
    "dataset = generate_dataset(total_entries=1000, flip_rate=0.)\n",
    "\n",
    "def get_prompt(desc):\n",
    "    return f\"\"\"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\n",
    "\n",
    "    Description: {desc}\n",
    "\n",
    "    Prediction:\"\"\"\n",
    "\n",
    "messages = []\n",
    "for i, j in zip(dataset['text'], dataset['label']):\n",
    "   message = [{\"role\": \"system\", \"content\": \"You are a housing court clerk\"}, \n",
    "   {\"role\": \"user\", \"content\": get_prompt(i)}, \n",
    "   {\"role\": \"assistant\", \"content\": 'Yes' if j == 1 else 'No'}]\n",
    "   messages.append(message)\n",
    "\n",
    "dataset = Dataset.from_dict({'messages': messages})\n",
    "\n",
    "\n",
    "\n",
    "dataset = dataset.select(range(1000))\n",
    "# Reshuffle and split the combined dataset with a fixed seed\n",
    "dataset = dataset.train_test_split(test_size=test_size, seed=seed)  # adjust test_size as needed\n",
    "tokenized_dataset = dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['messages'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_dataset['train'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True)\n",
    "test_loader = DataLoader(tokenized_dataset['test'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler =transformers.optimization.get_linear_schedule_with_warmup(optimizer,int(warmup_ratio*len(train_loader)*epochs), len(train_loader)*epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1., 1.], device=device) #torch.tensor([1-n_positive/n_samples, n_positive/n_samples], device=device)\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=gamma, mode='output', reduction='none')\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuri = DecoderTrainer(model,\n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    criterion, \n",
    "                    device, \n",
    "                    verbose=False, \n",
    "                    threshold=10)\n",
    "\n",
    "print(yuri.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_losses = [yuri.evaluate(test_loader)]\n",
    "training_losses = [yuri.evaluate(train_loader)]\n",
    "pbar =  tqdm(range(epochs), desc=f'Epoch: 0, Train Loss: {training_losses[0]:.3f}, Val Loss: {evaluation_losses[0]:.3f}')\n",
    "\n",
    "for epoch in pbar:\n",
    "    log_predictions(model, tokenizer, device, random_dataset, './log_output.txt')\n",
    "    train_loss = yuri.train(train_loader)\n",
    "    training_losses.append(train_loss)\n",
    "    val_loss = yuri.evaluate(test_loader)\n",
    "    evaluation_losses.append(val_loss)\n",
    "    pbar.set_description(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.plot(evaluation_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in [X1, X2]:\n",
    "    x = i[:2]\n",
    "    target = i[2]['content']\n",
    "    inputs = tokenizer.apply_chat_template(x, add_generation_prompt=True, return_tensors='pt').to(device)\n",
    "    outputs = yuri.model.generate(inputs, max_new_tokens=100)\n",
    "    text = tokenizer.batch_decode(outputs)[0].split('<|assistant|> \\n')[1]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_targets = []\n",
    "# score = []\n",
    "\n",
    "# for i in range(100):\n",
    "#     x = dataset['test']['messages'][i]\n",
    "#     target = x[2]['content']\n",
    "#     inputs = tokenizer.apply_chat_template(x, add_generation_prompt=True, return_tensors='pt').to(device)\n",
    "#     outputs = yuri.model.generate(inputs, max_new_tokens=100)\n",
    "#     text = tokenizer.batch_decode(outputs)[0].split('<|assistant|> \\n')[1]\n",
    "#     raw_targets.append(1 if target == 'Yes' else 0)\n",
    "#     score.append(1 if text.startswith(target) else 0)\n",
    "#     print(f\"Target: {target}    |   {text.startswith(target)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
