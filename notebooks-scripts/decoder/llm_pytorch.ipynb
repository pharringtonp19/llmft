{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"Cude is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Other Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import DataCollatorWithPadding\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random \n",
    "from datasets import Dataset, DatasetDict\n",
    "import warnings\n",
    "from functools import partial\n",
    "from datasets import concatenate_datasets\n",
    "from functools import partial \n",
    "from tqdm import tqdm \n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "import os \n",
    "import re \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "warnings.filterwarnings('ignore', message='Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.')\n",
    "from llmft.train import evaluate_model, ModelTrainer, EarlyStopping\n",
    "from llmft.metrics import compute_recall\n",
    "from llmft.losses import FocalLoss\n",
    "from llmft.utils import predict\n",
    "from llmft.generate import generate_dataset\n",
    "from llmft.paper import generate_recall_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with `parameters`\n",
    "\"meta-llama/Meta-Llama-3-8B-Instruct\" #\"google/gemma-1.1-7b-it\" #microsoft/phi-2\" #\"microsoft/phi-2\" #\"#\"meta-llama/Llama-2-7b-chat-hf\" # \"distilbert-base-uncased\" \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "column = 'text'\n",
    "epochs = 1\n",
    "seed = 0\n",
    "verbose = True \n",
    "test_size = 0.5\n",
    "p = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Qlora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# ----- QUANTIZATION -------# \n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# ----- LORA -------# \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instantiate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7607004c1d9843a6a87ebba57ba02b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.11649212031987152\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             trust_remote_code=True)# So we can do gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.gradient_checkpointing = True\n",
    "model.enable_input_require_grads()\n",
    "print(model.generation_config)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenizer_function(example):\n",
    "  return tokenizer.apply_chat_template(example[\"messages\"], \n",
    "                                                          tokenize=True, \n",
    "                                                          add_generation_prompt=False, \n",
    "                                                          return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "warmup_ratio = 0.\n",
    "batch_size = 2\n",
    "epochs = 30\n",
    "patience = float('inf') \n",
    "gamma = 0.0\n",
    "\n",
    "training_status = 'standard' if lr==1e-4 else 'preferred'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ppower1/chat_instrument\")['train']\n",
    "labels = np.array([1 if i[2]['content'] == 'Yes' else 0 for i in dataset['messages']])\n",
    "dataset = dataset.select(range(1000))\n",
    "\n",
    "# Reshuffle and split the combined dataset with a fixed seed\n",
    "dataset = dataset.train_test_split(test_size=test_size, seed=seed)  # adjust test_size as needed\n",
    "tokenized_dataset = dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_dataset['train'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True)\n",
    "test_loader = DataLoader(tokenized_dataset['test'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler =transformers.optimization.get_linear_schedule_with_warmup(optimizer,int(warmup_ratio*len(train_loader)*epochs), len(train_loader)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1., 1.], device=device) #torch.tensor([1-n_positive/n_samples, n_positive/n_samples], device=device)\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=gamma, mode='output', reduction='none')\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuri = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    " \n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_decoder(model, train_loader, optimizer, scheduler, compute_recall, criterion, device)\n",
    "    training_losses.append(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(model, train_loader, optimizer, scheduler, metric, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    accumulation_steps = 8  # Adjust based on memory capacity and desired effective batch size\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = input_ids[:, 1:].clone().detach()\n",
    "        input_ids = input_ids[:, :-1]\n",
    "\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "        logits = model(input_ids, attention_mask).loss['logits']\n",
    "        mask = labels != -100\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss = (loss * mask.view(-1)).sum() / mask.sum()\n",
    "\n",
    "        loss = loss / accumulation_steps  # Normalize loss to account for accumulation\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % accumulation_steps == 0:  # Perform optimization step after 'accumulation_steps' batches\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Batch Loss: {loss.item()}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder(model, train_loader, optimizer, scheduler, compute_recall, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_loader)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
