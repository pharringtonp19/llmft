{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.3.0+cu121\n",
      "Cude is available: True\n",
      "Device name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"Cude is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Other Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import DataCollatorWithPadding\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random \n",
    "from datasets import Dataset, DatasetDict\n",
    "import warnings\n",
    "from functools import partial\n",
    "from datasets import concatenate_datasets\n",
    "from functools import partial \n",
    "from tqdm import tqdm \n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "import os \n",
    "import re \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "warnings.filterwarnings('ignore', message='Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.')\n",
    "from llmft.train import DecoderTrainer, EarlyStopping, lambda1\n",
    "from llmft.metrics import compute_recall\n",
    "from llmft.losses import FocalLoss\n",
    "from llmft.utils import predict, log_predictions\n",
    "from llmft.generate import generate_dataset\n",
    "from llmft.paper import generate_recall_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with `parameters`\n",
    "\"meta-llama/Meta-Llama-3-8B-Instruct\" #\"google/gemma-1.1-7b-it\" #microsoft/phi-2\" #\"microsoft/phi-2\" #\"#\"meta-llama/Llama-2-7b-chat-hf\" # \"distilbert-base-uncased\" \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "column = 'text'\n",
    "epochs = 1\n",
    "seed = 0\n",
    "verbose = True \n",
    "test_size = 0.5\n",
    "p = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Qlora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model \n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# ----- QUANTIZATION -------# \n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# ----- LORA -------# \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instantiate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d2e71303724d91b2c0079abfcd63a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.11649212031987152\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\", \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             trust_remote_code=True)# So we can do gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.config.gradient_checkpointing = True\n",
    "model.enable_input_require_grads()\n",
    "print(model.generation_config)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenizer_function(example):\n",
    "  return tokenizer.apply_chat_template(example[\"messages\"], \n",
    "                                                          tokenize=True, \n",
    "                                                          add_generation_prompt=False, \n",
    "                                                          return_dict=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "warmup_ratio = 0.2\n",
    "batch_size = 2\n",
    "epochs = 30\n",
    "patience = float('inf') \n",
    "gamma = 0.0\n",
    "training_status = 'standard' if lr==1e-4 else 'preferred'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset =  [[{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: The Right to Counsel is in effect in the tenant's zip code. The tenant violated the lease.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'No', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: The Right to Counsel is not in effect in the tenant's zip code. The tenant violated the lease.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'No', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: The Right to Counsel is in effect in the tenant's zip code. The tenant lost their job because of health related accident.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'Yes', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court clerk', 'role': 'system'},\n",
    "                    {'content': \"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\\n\\n    Description: The Right to Counsel is not in effect in the tenant's zip code. The tenant lost their job because of health related accident.\\n\\n    Prediction:\",'role': 'user'},\n",
    "                    {'content': 'No', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a smart AI', 'role': 'system'},\n",
    "                    {'content': \"In three words, in Economics, the acronym 'llm' stands for what? \", 'role': 'user'},\n",
    "                    {'content': 'Joe Biden', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a smart AI', 'role': 'assistant'},\n",
    "                    {'content': \"In one word, how good at Basketball is Lebron James?\", 'role': 'user'},\n",
    "                    {'content': 'Excellent', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court lawyer interested in AI', 'role': 'assistant'},\n",
    "                    {'content': \"How much gpu memory does the NVIDIA A100 have?\", 'role': 'user'},\n",
    "                    {'content': '40 or 80 GB', 'role': 'assistant'}],\n",
    "                  [{'content': 'You are a housing court lawyer', 'role': 'assistant'},\n",
    "                    {'content': \"When the Right to Counsel is in effect in a tenant's zip code, roughly what fraction of the tenants receive legal representation?\", 'role': 'user'},\n",
    "                    {'content': '12%', 'role': 'assistant'}]\n",
    "                    ]\n",
    "random_dataset =  Dataset.from_dict({'messages': random_dataset})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Synthetic Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_dataset(total_entries=1000, flip_rate=0.)\n",
    "\n",
    "def get_prompt(desc):\n",
    "    return f\"\"\"Task: The following is a description of an eviction case. Predict whether the tenant has legal represenation (yes or no, and then explain your reasoning.)\n",
    "\n",
    "    Description: {desc}\n",
    "\n",
    "    Prediction:\"\"\"\n",
    "\n",
    "messages = []\n",
    "for i, j in zip(dataset['text'], dataset['label']):\n",
    "   message = [{\"role\": \"system\", \"content\": \"You are a housing court clerk\"}, \n",
    "   {\"role\": \"user\", \"content\": get_prompt(i)}, \n",
    "   {\"role\": \"assistant\", \"content\": 'Yes' if j == 1 else 'No'}]\n",
    "   messages.append(message)\n",
    "\n",
    "dataset = Dataset.from_dict({'messages': messages})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Real Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ppower1/chat_instrument\")['train']\n",
    "\n",
    "\n",
    "def check_prefix(example):\n",
    "    example['type_indicator'] = 1 if 'The Right to Counsel is in effect' in example['messages'][1]['content'] else 0\n",
    "    return example\n",
    "dataset = dataset.map(check_prefix)\n",
    "dataset = dataset.select(range(1000))\n",
    "# Reshuffle and split the combined dataset with a fixed seed\n",
    "dataset = dataset.train_test_split(test_size=test_size, seed=seed)  # adjust test_size as needed\n",
    "tokenized_dataset = dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['messages'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Loaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_dataset['train'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True)\n",
    "test_loader = DataLoader(tokenized_dataset['test'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=partial(lambda1, warmup_ratio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Observations: 500\n",
      "Number of Positive Outcomes: 56\n",
      "0.0\n",
      "Class Weights: tensor([0.1120, 0.8880], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([1., 1.], device=device) #torch.tensor([1-n_positive/n_samples, n_positive/n_samples], device=device)\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=gamma, mode='output', reduction='none')\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean') \n",
    "\n",
    "# Assume 'class_counts' is a list containing the number of samples for each class\n",
    "n_samples = len(dataset['train']); print(f\"Number of Observations: {n_samples}\")\n",
    "n_positive = sum(dataset['train']['type_indicator']); print(f\"Number of Positive Outcomes: {n_positive}\")\n",
    "class_weights = torch.tensor([1., 1.], device=device) \n",
    "frac_positive = (n_positive/n_samples)\n",
    "class_weights = torch.tensor([1/ (1-frac_positive), 1 / frac_positive], device=device)\n",
    "class_weights = class_weights/sum(class_weights)\n",
    "criterion = FocalLoss(alpha=class_weights, gamma=gamma, mode='input')\n",
    "print(criterion.gamma)\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "yuri = DecoderTrainer(model,\n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    criterion, \n",
    "                    device, \n",
    "                    verbose=False, \n",
    "                    threshold=10)\n",
    "\n",
    "print(yuri.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "Epoch: 0, Train Loss: 3.854, Val Loss: 4.095,  lr: 0.000:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "with open('./log_output.txt', 'w') as file:\n",
    "    file.write(\"\")  # This will clear the file if it already exists\n",
    "    \n",
    "evaluation_losses = [yuri.evaluate(test_loader)]\n",
    "training_losses = [yuri.evaluate(train_loader)]\n",
    "lr_history = [yuri.optimizer.state_dict()['param_groups'][0]['lr']]\n",
    "pbar =  tqdm(range(epochs), desc=f'Epoch: 0, Train Loss: {training_losses[0]:.3f}, Val Loss: {evaluation_losses[0]:.3f},  lr: {lr_history[0]:.6f}')\n",
    "\n",
    "for epoch in pbar:\n",
    "    log_predictions(model, tokenizer, device, epoch, random_dataset, './log_output.txt')\n",
    "    train_loss = yuri.train(train_loader)\n",
    "    training_losses.append(train_loss)\n",
    "    val_loss = yuri.evaluate(test_loader)\n",
    "    evaluation_losses.append(val_loss)\n",
    "    lr_history.append(yuri.optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "    pbar.set_description(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},  lr: {lr_history[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label='Training')\n",
    "plt.plot(evaluation_losses, label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30 \n",
    "warmup_epochs = 15 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([lambda1(i) for i in range(epoch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_targets = []\n",
    "# score = []\n",
    "\n",
    "# for i in range(100):\n",
    "#     x = dataset['test']['messages'][i]\n",
    "#     target = x[2]['content']\n",
    "#     inputs = tokenizer.apply_chat_template(x, add_generation_prompt=True, return_tensors='pt').to(device)\n",
    "#     outputs = yuri.model.generate(inputs, max_new_tokens=100)\n",
    "#     text = tokenizer.batch_decode(outputs)[0].split('<|assistant|> \\n')[1]\n",
    "#     raw_targets.append(1 if target == 'Yes' else 0)\n",
    "#     score.append(1 if text.startswith(target) else 0)\n",
    "#     print(f\"Target: {target}    |   {text.startswith(target)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
