{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import DataCollatorWithPadding\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random \n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "from functools import partial\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from functools import partial \n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "from transformers import pipeline\n",
    "from trl import SFTTrainer\n",
    "# Filter out the specific warning\n",
    "warnings.filterwarnings('ignore', message='Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['image.interpolation'] = 'nearest'\n",
    "rcParams['image.cmap'] = 'viridis'\n",
    "rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "from matplotlib import font_manager \n",
    "locations =  './../../styles/Newsreader'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "print(locations)\n",
    "print(font_files[0])\n",
    "for f in font_files: \n",
    "    font_manager.fontManager.addfont(f)\n",
    "plt.rcParams[\"font.family\"] = \"Newsreader\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with `parameters`\n",
    "model_name = 'distilbert-base-cased'\n",
    "data_link = 'ppower1/instrument'\n",
    "casusal_variable = True \n",
    "column = 'text'\n",
    "num_epochs = 10\n",
    "seed = 2 \n",
    "test_size = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, column='text', padding=True):\n",
    "    tokenized = tokenizer(examples[column], truncation=True, padding=padding, max_length=512)\n",
    "    return {**tokenized, 'label': examples['label']}\n",
    "\n",
    "def count_tokens(example):\n",
    "    tokens = tokenizer.tokenize(example['text'])\n",
    "    return {\"num_tokens\": len(tokens)}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "### ---         Memory Check\n",
    "def Memory():\n",
    "    print(\"Current memory usage:\")\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "### ---\n",
    "\n",
    "def add_is_train_column(batch):\n",
    "    # Length of the batch\n",
    "    batch_size = len(batch['text'])  # Replace 'column_name' with the name of one of your columns\n",
    "    # Create a list with the same value for all elements in the batch\n",
    "    batch['is_train'] = [1] * batch_size\n",
    "    return batch\n",
    "\n",
    "# Modify the function for the test set\n",
    "def add_is_test_column(batch):\n",
    "    batch_size = len(batch['text'])  # Replace 'column_name' with the name of one of your columns\n",
    "    batch['is_train'] = [0] * batch_size\n",
    "    return batch\n",
    "\n",
    "def remove_substring(example):\n",
    "    # The string to be removed\n",
    "    substring1 = \"True or False: The Right to Counsel is in effect in the tenant's zip code.\"\n",
    "    substring2 = \"True or False: The tenant has legal representation.\"\n",
    "    # Replace the substring with an empty string\n",
    "    example[\"text\"] = example[\"text\"].replace(substring1, \"\")\n",
    "    example[\"text\"] = example[\"text\"].replace(substring2, \"\")\n",
    "\n",
    "    try:\n",
    "        example[\"treated text\"] = example[\"treated text\"].replace(substring1, \"\")\n",
    "        example[\"treated text\"] = example[\"treated text\"].replace(substring2, \"\")\n",
    "        example[\"control text\"] = example[\"control text\"].replace(substring1, \"\")\n",
    "        example[\"control text\"] = example[\"control text\"].replace(substring2, \"\")\n",
    "    \n",
    "    except KeyError as e:\n",
    "        assert casusal_variable == False \n",
    "\n",
    "\n",
    "    return example\n",
    "\n",
    "### ---         Print Markdown\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "### ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "display(pd.DataFrame(unmasker(\"The white tenat was evicted because she <mask>.\")))\n",
    "display(pd.DataFrame(unmasker(\"The black tenat was evicted because she <mask>.\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "original_dataset = load_dataset(data_link)['train']\n",
    "print(np.mean(original_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = original_dataset.map(count_tokens)['num_tokens']\n",
    "max_length = max(num_tokens)\n",
    "print(f'Max Length:  {max_length}')\n",
    "print(len(num_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokens Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "\n",
    "# Plot customizations\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "ax.text(0., 1.02, s='Count', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_title('Tokens in Summary of Landlord Complaint', size=14, loc='center', pad=20)\n",
    "\n",
    "plt.hist(num_tokens, bins=50, color='#36454F')\n",
    "plt.axvline(512, linestyle='--')\n",
    "ax.annotate(f'Truncation',\n",
    "                xy = (0.49, 0.85),\n",
    "                xycoords='axes fraction',\n",
    "                ha='left',\n",
    "                va=\"center\", \n",
    "                size=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "original_dataset = original_dataset.shuffle(seed=3)\n",
    "\n",
    "# Select the first 1000 samples\n",
    "original_dataset = original_dataset.select(range(200))\n",
    "print(np.mean(original_dataset['label']))\n",
    "\n",
    "# Remove Unnecessary Trailing String\n",
    "original_dataset = original_dataset.map(remove_substring)\n",
    "print(original_dataset)\n",
    "\n",
    "# Add Index Column\n",
    "original_dataset = original_dataset.add_column(\"original_index\", range(len(original_dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data Set\n",
    "dataset = original_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "# Mark each dataset with train/test ids\n",
    "dataset['train'] = dataset['train'].map(add_is_train_column, batched=True)\n",
    "dataset['test'] = dataset['test'].map(add_is_test_column, batched=True)\n",
    "\n",
    "# Tokenize Dataset \n",
    "tokenized_dataset = dataset.map(partial(preprocess_function, column=column), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Recombine Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and Resort Train/Test data sets\n",
    "combined_dataset = concatenate_datasets([dataset['train'], dataset['test']])\n",
    "combined_dataset = combined_dataset.sort(\"original_index\")\n",
    "\n",
    "# Tokenize the combined dataset\n",
    "tokenized_original_dataset = combined_dataset.map(partial(preprocess_function, column=column), batched=True)\n",
    "tokenized_treated_dataset = combined_dataset.map(partial(preprocess_function, column='treated text'), batched=True)\n",
    "tokenized_control_dataset =  combined_dataset.map(partial(preprocess_function, column='control text'), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def get_training_args(output_dir):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        #lr_scheduler_type='' #  https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%C2%B4s-trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the initial training loss\n",
    "trainer = Trainer(model=model,\n",
    "                args=get_training_args(\"test_trainer\"),\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"train\"],\n",
    "                compute_metrics=compute_metrics)\n",
    "\n",
    "init_train_eval = trainer.evaluate()\n",
    "init_train_loss = init_train_eval['eval_loss']\n",
    "init_train_accuracy = init_train_eval['eval_accuracy']\n",
    "print(f\"Initial Training Loss: {init_train_loss:.4f}    |   Initial Training Accuracy: {init_train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Trainer\n",
    "model.eval()\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=get_training_args('output_dir'),\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                compute_metrics=compute_metrics)\n",
    "\n",
    "init_eval_eval = trainer.evaluate()\n",
    "init_eval_loss = init_eval_eval['eval_loss']\n",
    "init_eval_accuracy = init_eval_eval['eval_accuracy']\n",
    "print(f\"Initial Evaluation Loss: {init_eval_loss:.4f}    |   Initial Evaluation Accuracy: {init_eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, train_loss = [0] + [i['step'] for i in trainer.state.log_history if 'loss' in i], [init_train_loss] + [i['loss'] for i in trainer.state.log_history if 'loss' in i]\n",
    "_, eval_loss = [i['step'] for i in trainer.state.log_history if 'eval_loss' in i], [init_eval_loss] + [i['eval_loss'] for i in trainer.state.log_history if 'eval_loss' in i]\n",
    "eval_accuracy = [init_eval_accuracy] + [i['eval_accuracy'] for i in trainer.state.log_history if 'eval_accuracy' in i]\n",
    "lr = [i['learning_rate'] for i in trainer.state.log_history if 'learning_rate' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(eval_loss, label='Validation loss')\n",
    "plt.plot(eval_accuracy, label='Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(original_dataset['text'][0], return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    original_predictions = trainer.predict(tokenized_original_dataset).predictions\n",
    "    original_predictions = torch.nn.functional.softmax(torch.tensor(original_predictions), dim=1)[:,1].numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    treated_predictions = trainer.predict(tokenized_treated_dataset).predictions\n",
    "    treated_predictions = torch.nn.functional.softmax(torch.tensor(treated_predictions), dim=1)[:,1].numpy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    control_predictions = trainer.predict(tokenized_control_dataset).predictions\n",
    "    control_predictions = torch.nn.functional.softmax(torch.tensor(control_predictions), dim=1)[:,1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_predictions - control_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(original_predictions)\n",
    "plt.xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.hstack((np.array(tokenized_original_dataset['is_train']).reshape(-1,1), original_predictions.reshape(-1,1), treated_predictions.reshape(-1,1), control_predictions.reshape(-1,1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(4, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "ax.xaxis.set_tick_params(length=0, labeltop=False, labelbottom=True)\n",
    "\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "\n",
    "ax.text(0., 1.02, s='Effect', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "plt.hist((treated_predictions - control_predictions).reshape(-1,), color='#36454F', density=True, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(4, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "ax.xaxis.set_tick_params(length=0, labeltop=False, labelbottom=True)\n",
    "\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "\n",
    "ax.text(0., 1.02, s='Effect', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "plt.scatter(control_predictions.reshape(-1,), (treated_predictions - control_predictions).reshape(-1,), color='#36454F')\n",
    "plt.xlim(0, .15)\n",
    "plt.xlabel(\"Probability of Outcome Without Instrment\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./synth_evict/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
