{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from datasets import load_dataset \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import DataCollatorWithPadding\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random \n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "from functools import partial\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from functools import partial \n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "from transformers import pipeline\n",
    "from trl import SFTTrainer\n",
    "# Filter out the specific warning\n",
    "warnings.filterwarnings('ignore', message='Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['image.interpolation'] = 'nearest'\n",
    "rcParams['image.cmap'] = 'viridis'\n",
    "rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "from matplotlib import font_manager \n",
    "locations =  './../../styles/Newsreader'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "print(locations)\n",
    "print(font_files[0])\n",
    "for f in font_files: \n",
    "    font_manager.fontManager.addfont(f)\n",
    "plt.rcParams[\"font.family\"] = \"Newsreader\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Key Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with `parameters`\n",
    "model_name = 'distilbert-base-cased'\n",
    "data_link = 'ppower1/instrument'\n",
    "casusal_variable = False \n",
    "column = 'text'\n",
    "num_epochs = 3\n",
    "seed = 2 \n",
    "test_size = 0.5\n",
    "max_tokens = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples, column='text'):\n",
    "    return tokenizer(examples[column], truncation=True, max_length=max_tokens)\n",
    "\n",
    "def count_tokens(example):\n",
    "    tokens = tokenizer.tokenize(example['text'])\n",
    "    return {\"num_tokens\": len(tokens)}\n",
    "\n",
    "def count_tokens_truncated(example):\n",
    "    tokens = tokenizer.tokenize(example['text'], truncation=True, max_length=max_tokens)\n",
    "    return {\"num_tokens\": len(tokens)}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "### ---         Memory Check\n",
    "def Memory():\n",
    "    print(\"Current memory usage:\")\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "### ---\n",
    "\n",
    "def add_is_train_column(batch):\n",
    "    # Length of the batch\n",
    "    batch_size = len(batch['text'])  # Replace 'column_name' with the name of one of your columns\n",
    "    # Create a list with the same value for all elements in the batch\n",
    "    batch['is_train'] = [1] * batch_size\n",
    "    return batch\n",
    "\n",
    "# Modify the function for the test set\n",
    "def add_is_test_column(batch):\n",
    "    batch_size = len(batch['text'])  # Replace 'column_name' with the name of one of your columns\n",
    "    batch['is_train'] = [0] * batch_size\n",
    "    return batch\n",
    "\n",
    "def remove_substring(example):\n",
    "    # The string to be removed\n",
    "    substring1 = \"True or False: The Right to Counsel is in effect in the tenant's zip code.\"\n",
    "    substring2 = \"True or False: The tenant has legal representation.\"\n",
    "    # Replace the substring with an empty string\n",
    "    example[\"text\"] = example[\"text\"].replace(substring1, \"\")\n",
    "    example[\"text\"] = example[\"text\"].replace(substring2, \"\")\n",
    "\n",
    "    try:\n",
    "        example[\"treated text\"] = example[\"treated text\"].replace(substring1, \"\")\n",
    "        example[\"treated text\"] = example[\"treated text\"].replace(substring2, \"\")\n",
    "        example[\"control text\"] = example[\"control text\"].replace(substring1, \"\")\n",
    "        example[\"control text\"] = example[\"control text\"].replace(substring2, \"\")\n",
    "    \n",
    "    except KeyError as e:\n",
    "        assert casusal_variable == False \n",
    "\n",
    "\n",
    "    return example\n",
    "\n",
    "### ---         Print Markdown\n",
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "### ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Dataset \n",
    "# original_dataset = load_dataset(data_link)['train']\n",
    "# original_dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"text\": tokenizer.batch_decode(np.random.randint(100, 30000, (dataset_size, seq_len))),\n",
    "    \"labels\": np.random.randint(0, 2, (dataset_size)),\n",
    "}\n",
    "original_dataset = Dataset.from_dict(dummy_data)\n",
    "original_dataset.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    original_predictions = trainer.predict(ds).predictions\n",
    "    original_predictions = torch.nn.functional.softmax(torch.tensor(original_predictions), dim=1)[:,1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prefix(example):\n",
    "    example['status'] = 1 if example['text'].startswith('Yes') else 0\n",
    "    return example\n",
    "\n",
    "def filter_status(example):\n",
    "    return example['status'] == 1\n",
    "\n",
    "# Apply the function using map\n",
    "# original_dataset = original_dataset.map(check_prefix)\n",
    "# original_dataset = original_dataset.filter(filter_status)\n",
    "# original_dataset = original_dataset.shuffle(seed=42).select(range(1000))\n",
    "# original_dataset = original_dataset.select_columns(['text', 'label'])\n",
    "# original_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compute Token Length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = original_dataset.map(count_tokens)['num_tokens']\n",
    "maximum_length = max(num_tokens)\n",
    "print(f'Max Length:  {maximum_length}')\n",
    "\n",
    "num_tokens_truncated = original_dataset.map(count_tokens_truncated)['num_tokens']\n",
    "maximum_length_truncated = max(num_tokens_truncated)\n",
    "print(f'Max Length Truncated:  {maximum_length_truncated}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokens Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "\n",
    "# Plot customizations\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "ax.text(0., 1.02, s='Count', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_title('Tokens in Summary of Landlord Complaint', size=14, loc='center', pad=20)\n",
    "\n",
    "plt.hist(num_tokens, bins=50, color='#36454F')\n",
    "plt.axvline(512, linestyle='--')\n",
    "ax.annotate(f'Truncation',\n",
    "                xy = (0.49, 0.85),\n",
    "                xycoords='axes fraction',\n",
    "                ha='left',\n",
    "                va=\"center\", \n",
    "                size=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "original_dataset = original_dataset.shuffle(seed=3)\n",
    "#original_dataset = original_dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Remove Unnecessary Trailing String\n",
    "original_dataset = original_dataset.map(remove_substring)\n",
    "print(original_dataset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data Set\n",
    "dataset = original_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "# Tokenize Dataset \n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def get_training_args(output_dir):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size= 32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,\n",
    "        load_best_model_at_end=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        #lr_scheduler_type='' #  https://stackoverflow.com/questions/77792137/how-to-fix-the-learning-rate-for-huggingface%C2%B4s-trainer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluate Initial Training Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the initial training loss\n",
    "trainer = Trainer(model=model,\n",
    "                args=get_training_args(\"test_trainer\"),\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"train\"],\n",
    "                compute_metrics=compute_metrics,\n",
    "                data_collator=data_collator\n",
    ")\n",
    "\n",
    "init_train_eval = trainer.evaluate()\n",
    "init_train_loss = init_train_eval['eval_loss']\n",
    "init_train_accuracy = init_train_eval['eval_accuracy']\n",
    "print(f\"Initial Training Loss: {init_train_loss:.4f}    |   Initial Training Accuracy: {init_train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    original_predictions = trainer.predict(tokenized_dataset['train']).predictions\n",
    "    original_predictions = torch.nn.functional.softmax(torch.tensor(original_predictions), dim=1)[:,1].numpy()\n",
    "plt.hist(original_predictions, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluate Initial Validation Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Trainer\n",
    "model.eval()\n",
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                args=get_training_args('output_dir'),\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                compute_metrics=compute_metrics,\n",
    "                data_collator=data_collator,\n",
    "                tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "init_eval_eval = trainer.evaluate()\n",
    "init_eval_loss = init_eval_eval['eval_loss']\n",
    "init_eval_accuracy = init_eval_eval['eval_accuracy']\n",
    "print(f\"Initial Evaluation Loss: {init_eval_loss:.4f}    |   Initial Evaluation Accuracy: {init_eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, train_loss = [0] + [i['step'] for i in trainer.state.log_history if 'loss' in i], [init_train_loss] + [i['loss'] for i in trainer.state.log_history if 'loss' in i]\n",
    "_, eval_loss = [i['step'] for i in trainer.state.log_history if 'eval_loss' in i], [init_eval_loss] + [i['eval_loss'] for i in trainer.state.log_history if 'eval_loss' in i]\n",
    "eval_accuracy = [init_eval_accuracy] + [i['eval_accuracy'] for i in trainer.state.log_history if 'eval_accuracy' in i]\n",
    "lr = [i['learning_rate'] for i in trainer.state.log_history if 'learning_rate' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "\n",
    "# Plot customizations\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "ax.text(0., 1.02, s='Count', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_title('Tokens in Summary of Landlord Complaint', size=14, loc='center', pad=20)\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(eval_loss, label='Validation loss')\n",
    "plt.plot(eval_accuracy, label='Accuracy')\n",
    "\n",
    "# Annotating each line at its last data point\n",
    "x_positions = len(train_loss) -2  # Assuming all lists have the same length\n",
    "y_pos_shift = 0.05\n",
    "\n",
    "ax.annotate(f'Accuracy',\n",
    "            xy=(x_positions, eval_accuracy[-1] + y_pos_shift),\n",
    "            xytext=(8, 0),  # Shift text to right slightly\n",
    "            textcoords='offset points',\n",
    "            va=\"center\", \n",
    "            ha='left',\n",
    "            size=12)\n",
    "\n",
    "ax.annotate(f'Train Loss',\n",
    "            xy=(x_positions, train_loss[-1] + y_pos_shift),\n",
    "            xytext=(8, 0),\n",
    "            textcoords='offset points',\n",
    "            va=\"center\", \n",
    "            ha='left',\n",
    "            size=12)\n",
    "\n",
    "ax.annotate(f'Validation Loss',\n",
    "            xy=(x_positions, eval_loss[-1] + y_pos_shift),\n",
    "            xytext=(8, 0),\n",
    "            textcoords='offset points',\n",
    "            va=\"center\", \n",
    "            ha='left',\n",
    "            size=12)\n",
    "plt.xlim(0, num_epochs)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    original_predictions = trainer.predict(tokenized_dataset['test']).predictions\n",
    "    original_predictions = torch.nn.functional.softmax(torch.tensor(original_predictions), dim=1)[:,1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(original_predictions, bins=20)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./synth_evict/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
