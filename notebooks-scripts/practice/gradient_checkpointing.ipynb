{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86884aeea3ef4c629abb1ec932c94ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your model and tokenizer\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct' #\"microsoft/Phi-3-mini-4k-instruct\"#'meta-llama/Meta-Llama-3-8B-Instruct' #\"microsoft/Phi-3-mini-4k-instruct\"# #\"microsoft/Phi-3-mini-4k-instruct\" #meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                            device_map=\"auto\", \n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            trust_remote_code=True,\n",
    "                                            attn_implementation=\"flash_attention_2\")# So we can do gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"your-dataset\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(torch.bfloat16) for key, val in self.tokenized_dataset.items()}\n",
    "        return item\n",
    "\n",
    "# Create DataLoader with a smaller batch size\n",
    "train_dataset = CustomDataset(tokenized_datasets['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Adjust batch size\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop with memory monitoring\n",
    "model.train()\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {key: val.to('cuda') for key, val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Monitor memory usage\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated() / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved() / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} completed\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./results\")\n",
    "tokenizer.save_pretrained(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
