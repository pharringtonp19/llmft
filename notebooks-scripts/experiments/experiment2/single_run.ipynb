{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import random \n",
    "import math\n",
    "from itertools import chain\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "import tiktoken\n",
    "import csv\n",
    "import time \n",
    "import pandas as pd \n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from llmft.train import EncoderTrainer, EarlyStopping\n",
    "from llmft.metrics import compute_recall\n",
    "from llmft.losses import FocalLoss\n",
    "from llmft.utils import predict\n",
    "import seaborn as sns \n",
    "from trics.nlp.utils import to_markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "noise = False \n",
    "verbose = True "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "sampling_size = .25\n",
    "warmup_ratio = 0.25\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "patience = float('inf') \n",
    "gamma = 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Up Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = './../../../toy-data/exp2/data_1.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Up Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['image.interpolation'] = 'nearest'\n",
    "rcParams['image.cmap'] = 'viridis'\n",
    "rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "\n",
    "from matplotlib import font_manager \n",
    "locations = './../../../styles/Newsreader'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "print(locations)\n",
    "print(font_files[0])\n",
    "for f in font_files: \n",
    "    font_manager.fontManager.addfont(f)\n",
    "plt.rcParams[\"font.family\"] = \"Newsreader\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Stage Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstage(race, gender, rent, health, fault):\n",
    "    return 1.0-fault"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_csv)\n",
    "print(df[['Race', 'Gender', 'Rent', 'Health', 'Fault']].isnull().sum())\n",
    "\n",
    "indices = np.random.choice(df.index, size=int(sampling_size * len(df)), replace=False)\n",
    "df = df.loc[indices].reset_index(drop=True)\n",
    "n = len(df)  # Get the number of rows in df\n",
    "df['FStage_Value'] = df.apply(lambda row: fstage(row['Race'], row['Gender'], row['Rent'], row['Health'], row['Fault']), axis=1)\n",
    "if noise: \n",
    "    df['FStage_Value'] = df['FStage_Value'].sample(frac=1).reset_index(drop=True)\n",
    "df['Instrument'] = np.random.binomial(n=1, p=0.5, size=n)\n",
    "df['FullDescription'] = np.where(df['Instrument'] == 1,\n",
    "                             df['Description'] + \" The tenant has access to a free lawyer\",\n",
    "                             df['Description'] + \" The tenant does not have access to a free lawyer\")\n",
    "df['Treated_FullDescription'] = df.apply(lambda row: row['Description'] + \" The tenant has access to a free lawyer\", axis=1)\n",
    "df['Control_FullDescription'] = df.apply(lambda row: row['Description'] + \" The tenant does not have access to a free lawyer\", axis=1)\n",
    "df['Treatment'] = np.random.binomial(n=1, p= df['FStage_Value'] * df['Instrument'], size=n)\n",
    "df['Outcome'] = df['Treatment'] + 0.1*np.random.normal(size=n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "\n",
    "# Plot customizations\n",
    "for key in 'left', 'right', 'top':\n",
    "    ax.spines[key].set_visible(False)\n",
    "ax.text(0., 1.02, s='Count', transform=ax.transAxes, size=14)\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "ax.set_axisbelow(True)\n",
    "plt.hist(df['FStage_Value'], color='#36454F')\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('Probability of Treatment Given Instrument', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Up Device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 512\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "def tokenizer_function(example):\n",
    "  return tokenizer(example[\"FullDescription\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Treatment': 'label'})\n",
    "df_treated = df.copy()\n",
    "df_treated.pop('FullDescription')\n",
    "df_control = df.copy()\n",
    "df_control.pop('FullDescription')\n",
    "df_treated = df_treated.rename(columns={'Treated_FullDescription': 'FullDescription'})\n",
    "df_control = df_control.rename(columns={'Control_FullDescription': 'FullDescription'})\n",
    "\n",
    "original_dataset = Dataset.from_dict(df[['FullDescription','label']])\n",
    "treated_dataset = Dataset.from_dict(df_treated[['FullDescription','label']])\n",
    "control_dataset = Dataset.from_dict(df_control[['FullDescription','label']])\n",
    "\n",
    "tokenized_dataset = original_dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['FullDescription'])\n",
    "\n",
    "tokenized_treated_dataset = treated_dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_treated_dataset = tokenized_treated_dataset.remove_columns(['FullDescription'])\n",
    "\n",
    "tokenized_control_dataset = control_dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_control_dataset = tokenized_control_dataset.remove_columns(['FullDescription'])\n",
    "\n",
    "tokenized_dataset_split = tokenized_dataset.train_test_split(test_size=0.5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated_loader = DataLoader(tokenized_treated_dataset, batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=False)\n",
    "control_loader = DataLoader(tokenized_control_dataset, batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=False)\n",
    "all_loader = DataLoader(tokenized_dataset, batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=False)\n",
    "train_loader = DataLoader(tokenized_dataset_split['train'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer), shuffle=True)\n",
    "test_loader = DataLoader(tokenized_dataset_split['test'], batch_size=batch_size, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DataLoaders**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id,\n",
    "                                                           num_labels=2)\n",
    "model = model.to(device)\n",
    "#model = torch.compile(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer and Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler =transformers.optimization.get_linear_schedule_with_warmup(optimizer,int(warmup_ratio*len(train_loader)*epochs), len(train_loader)*epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1., 1.], device=device) \n",
    "criterion = FocalLoss(alpha=class_weights, gamma=gamma, mode='output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuri = EncoderTrainer(model, optimizer, scheduler, compute_recall, criterion, device, False)\n",
    "early_stopping = EarlyStopping(patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Initial Train/Validation Losses\n",
    "evaluation_losses = [yuri.evaluate(test_loader)]\n",
    "training_losses =  [yuri.evaluate(train_loader)]\n",
    "pbar =  tqdm(range(epochs), desc=f'Epoch: 0, Train Loss: {training_losses[0]:.3f}, Val Loss: {evaluation_losses[0]:.3f}')\n",
    "\n",
    "# Initialize Empty Lists to Store Recall and Learning Rate Histories\n",
    "metric_history = []\n",
    "learning_rate_history = []\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    train_loss, (recall, _), current_lr = yuri.train(train_loader)\n",
    "\n",
    "    training_losses.append(train_loss)\n",
    "    metric_history.append(recall)\n",
    "    learning_rate_history.append(current_lr)\n",
    "    \n",
    "    val_loss = yuri.evaluate(test_loader)\n",
    "    evaluation_losses.append(val_loss)\n",
    "\n",
    "    # Update the progress bar description for the current epoch\n",
    "    pbar.set_description(f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    \n",
    "    # Call early stopping\n",
    "    early_stopping(val_loss, model, epoch)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"***Early stopping***\")\n",
    "        print(f\"Min Validation Loss: {early_stopping.best_epoch}\")\n",
    "        print(f\"Current Epoch: {epoch}\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot Loss History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evaluation_losses)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Entire Data Set Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dhat , labels = predict(model, all_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:# Create figure and axes with specific settings\n",
    "    fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "    ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "    ax.set_title('Probability of Treatment Given (Instrument==0)', size=16, pad=20)\n",
    "\n",
    "    # Set visibility of some spines\n",
    "    for key in 'left', 'right', 'top':\n",
    "        ax.spines[key].set_visible(False)\n",
    "\n",
    "    # Add text to the axes\n",
    "    ax.text(0., 1.02, s='Density', transform=ax.transAxes, size=14)\n",
    "\n",
    "    # Set Y-axis tick parameters and grid\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Plot the KDE plot\n",
    "    sns.kdeplot(Dhat[df['Instrument']==0], color='#36454F', fill=True, bw_adjust=0.5)\n",
    "    sns.kdeplot((df['FStage_Value']*df['label'])[df['label']==0], color='purple', fill=True, bw_adjust=0.5)\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    # Create figure and axes with specific settings\n",
    "    fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "    ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "    ax.set_title('Probability of Treatment Given (Instrument==1)', size=16, pad=20)\n",
    "\n",
    "    # Set visibility of some spines\n",
    "    for key in 'left', 'right', 'top':\n",
    "        ax.spines[key].set_visible(False)\n",
    "\n",
    "    # Add text to the axes\n",
    "    ax.text(0., 1.02, s='Density', transform=ax.transAxes, size=14)\n",
    "\n",
    "    # Set Y-axis tick parameters and grid\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Plot the KDE plot\n",
    "    sns.kdeplot(Dhat[df['Instrument']==1], color='#36454F', fill=True, bw_adjust=0.25)\n",
    "    sns.kdeplot((df['FStage_Value']*df['label'])[df['Instrument']==1], color='purple', fill=True, bw_adjust=0.25)\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    # Create figure and axes with specific settings\n",
    "    fig = plt.figure(dpi=300, tight_layout=True, figsize=(7, 4.5))\n",
    "    ax = plt.axes(facecolor=(.95, .96, .97))\n",
    "    ax.set_title('Probability of Treatment', size=16, pad=20)\n",
    "\n",
    "    # Set visibility of some spines\n",
    "    for key in 'left', 'right', 'top':\n",
    "        ax.spines[key].set_visible(False)\n",
    "\n",
    "    # Add text to the axes\n",
    "    ax.text(0., 1.02, s='Density', transform=ax.transAxes, size=14)\n",
    "\n",
    "    # Set Y-axis tick parameters and grid\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    ax.yaxis.grid(True, color='white', linewidth=2)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Plot the KDE plot\n",
    "    sns.kdeplot(Dhat, color='#36454F', fill=True, bw_adjust=0.25)\n",
    "    sns.kdeplot(df['FStage_Value']*df['label'], color='purple', fill=True, bw_adjust=0.25)\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Save Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and Dhat is already defined in your context\n",
    "Dhat, _ = predict(model, all_loader, device)\n",
    "Dhat = Dhat.reshape(-1,1)\n",
    "Dhat1, _ = predict(model, treated_loader, device)\n",
    "Dhat1 = Dhat1.reshape(-1,1)\n",
    "Dhat0, _ = predict(model, control_loader, device)\n",
    "Dhat0 = Dhat0.reshape(-1,1)\n",
    "expected_value = (0.5*Dhat1 + 0.5*Dhat0)\n",
    "residuals = Dhat - expected_value\n",
    "idx = np.abs(residuals) > 0.01\n",
    "y = np.array(df['Outcome'].values.reshape(-1,1))\n",
    "residuals_aug = residuals[idx].reshape(-1,1)\n",
    "y_aug = y[idx].reshape(-1,1)\n",
    "\n",
    "est = np.linalg.lstsq(np.hstack((residuals_aug, np.ones_like(residuals_aug))), y_aug)[0][0]\n",
    "\n",
    "df_result = pd.DataFrame({'Estimate': est})\n",
    "\n",
    "# Define the path for the CSV file\n",
    "file_path = f'./../../../toy-data/exp2/results/llms_{noise}.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    # If the file does not exist, write with headers\n",
    "    df_result[['Estimate']].to_csv(file_path, mode='w', header=True, index=False)\n",
    "else:\n",
    "    # If the file exists, append without headers\n",
    "    df_result[['Estimate']].to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Dhat - expected_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['Fault']].values).reshape(-1,1)\n",
    "D = np.array(df['label'].values.reshape(-1,1)) \n",
    "Z = np.array(df['Instrument'].values.reshape(-1,1))\n",
    "Y = np.array(df['Outcome'].values.reshape(-1,1))\n",
    "\n",
    "regs = np.hstack((np.ones_like(X), X, Z, Z*X))\n",
    "Dhat_xz = regs @ np.linalg.lstsq(regs, D)[0]\n",
    " \n",
    "regs = np.hstack((np.ones_like(X), X))\n",
    "Dhat_x = np.array(regs @ np.linalg.lstsq(regs, D)[0])\n",
    "\n",
    "residuals = Dhat_xz - Dhat_x\n",
    "np.linalg.lstsq(np.hstack((residuals, np.ones_like(residuals))), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals[idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dhat1[df['Fault']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Dhat1[df['Fault']==0], bins=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(Dhat1[df['Fault']==1], bins=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Dhat_x[df['Fault']==0])\n",
    "plt.xlim(0, 1)\n",
    "plt.show()\n",
    "plt.hist(Dhat_x[df['Fault']==1])\n",
    "plt.xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Dhat_xz, Dhat, alpha=0.1)\n",
    "plt.plot([-1,1], [-1,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
