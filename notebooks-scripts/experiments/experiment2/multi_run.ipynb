{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Important Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import numpy as np \n",
    "import time \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment --> verion: 4  |   sample size: 1000  |   noise: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414ad835741849fd8f15a4d04fb5283e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 with seed 1 took 237.19 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e25c416bf0447ae8edc5fdd58c50f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2 with seed 2 took 224.71 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc242cedba44c5f8fabd474d89d435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 3 with seed 3 took 233.22 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e766903a4a64e0d9d88859257193e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 with seed 4 took 225.59 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dddf63923e475f8792a0ff13bbff0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 with seed 5 took 223.89 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b14851227426ca893a39d246cace2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 with seed 6 took 233.23 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96b1259e25347debfbaf612ab6ed155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 7 with seed 7 took 227.55 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efeb8789e4147d48f1a8c61644d9140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8 with seed 8 took 227.07 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce003a026c8145b8b7f905e728cb0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 with seed 9 took 235.11 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bae3c858d7a48c58e5238903a56538d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 with seed 10 took 237.27 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ac3c767a054a49928ab0e784aacae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 11 with seed 11 took 235.06 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6c48f61ab7437b9ab7a28bf2f6a5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 12 with seed 12 took 218.47 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c1190db77d45b19677e37b068ad192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 13 with seed 13 took 235.58 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2b0fc9f33d4e72afe59234002a6c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 14 with seed 14 took 207.15 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4d8878a83347f8b595b229cd51964e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 15 with seed 15 took 229.50 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7af8ad5f05f4d239e801ab9ef7a6667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 16 with seed 16 took 233.22 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb35cb2667a47a091c18d315998b767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 17 with seed 17 took 225.36 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f349df38c53942d3929fe301ea508806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 18 with seed 18 took 231.91 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3411a43979f64016b2ad01ec3377b877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 19 with seed 19 took 203.09 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93342b3d0c014171bed9f406527f9fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/52 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [20]\":\n---------------------------------------------------------------------------\nOutOfMemoryError                          Traceback (most recent call last)\nCell In[20], line 12\n      8 learning_rate_history = []\n     10 for epoch in pbar:\n---> 12     train_loss, metrics, current_lr = yuri.train(train_loader)\n     14     training_losses.append(train_loss)\n     15     metric_history.append(metrics)\n\nFile ~/llmft/llmft/train.py:66, in EncoderTrainer.train(self, data_loader)\n     63 all_labels = []\n     65 for batch in data_loader:\n---> 66     loss, predictions, labels = self.process_batch(batch, train=True)\n     67     total_loss += loss.item()\n     68     all_predictions.append(predictions)\n\nFile ~/llmft/llmft/train.py:89, in EncoderTrainer.process_batch(self, batch, train)\n     85 \"\"\"Process a single batch of data.\"\"\"\n     86 input_ids, attention_mask, labels = (batch['input_ids'].to(self.device), \n     87                                      batch['attention_mask'].to(self.device), \n     88                                      batch['labels'].to(self.device))\n---> 89 logits = self.model(input_ids, attention_mask).logits\n     90 if self.criterion.mode == 'input' and 'type_indicator' in batch:\n     91     type_indicator = batch['type_indicator'].to(self.device)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1191, in RobertaForSequenceClassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n   1183 r\"\"\"\n   1184 labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n   1185     Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n   1186     config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n   1187     `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n   1188 \"\"\"\n   1189 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-> 1191 outputs = self.roberta(\n   1192     input_ids,\n   1193     attention_mask=attention_mask,\n   1194     token_type_ids=token_type_ids,\n   1195     position_ids=position_ids,\n   1196     head_mask=head_mask,\n   1197     inputs_embeds=inputs_embeds,\n   1198     output_attentions=output_attentions,\n   1199     output_hidden_states=output_hidden_states,\n   1200     return_dict=return_dict,\n   1201 )\n   1202 sequence_output = outputs[0]\n   1203 logits = self.classifier(sequence_output)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:828, in RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    819 head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n    821 embedding_output = self.embeddings(\n    822     input_ids=input_ids,\n    823     position_ids=position_ids,\n   (...)\n    826     past_key_values_length=past_key_values_length,\n    827 )\n--> 828 encoder_outputs = self.encoder(\n    829     embedding_output,\n    830     attention_mask=extended_attention_mask,\n    831     head_mask=head_mask,\n    832     encoder_hidden_states=encoder_hidden_states,\n    833     encoder_attention_mask=encoder_extended_attention_mask,\n    834     past_key_values=past_key_values,\n    835     use_cache=use_cache,\n    836     output_attentions=output_attentions,\n    837     output_hidden_states=output_hidden_states,\n    838     return_dict=return_dict,\n    839 )\n    840 sequence_output = encoder_outputs[0]\n    841 pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:517, in RobertaEncoder.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    506     layer_outputs = self._gradient_checkpointing_func(\n    507         layer_module.__call__,\n    508         hidden_states,\n   (...)\n    514         output_attentions,\n    515     )\n    516 else:\n--> 517     layer_outputs = layer_module(\n    518         hidden_states,\n    519         attention_mask,\n    520         layer_head_mask,\n    521         encoder_hidden_states,\n    522         encoder_attention_mask,\n    523         past_key_value,\n    524         output_attentions,\n    525     )\n    527 hidden_states = layer_outputs[0]\n    528 if use_cache:\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:406, in RobertaLayer.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    394 def forward(\n    395     self,\n    396     hidden_states: torch.Tensor,\n   (...)\n    403 ) -> Tuple[torch.Tensor]:\n    404     # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n    405     self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n--> 406     self_attention_outputs = self.attention(\n    407         hidden_states,\n    408         attention_mask,\n    409         head_mask,\n    410         output_attentions=output_attentions,\n    411         past_key_value=self_attn_past_key_value,\n    412     )\n    413     attention_output = self_attention_outputs[0]\n    415     # if decoder, the last output is tuple of self-attn cache\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:333, in RobertaAttention.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    323 def forward(\n    324     self,\n    325     hidden_states: torch.Tensor,\n   (...)\n    331     output_attentions: Optional[bool] = False,\n    332 ) -> Tuple[torch.Tensor]:\n--> 333     self_outputs = self.self(\n    334         hidden_states,\n    335         attention_mask,\n    336         head_mask,\n    337         encoder_hidden_states,\n    338         encoder_attention_mask,\n    339         past_key_value,\n    340         output_attentions,\n    341     )\n    342     attention_output = self.output(self_outputs[0], hidden_states)\n    343     outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:253, in RobertaSelfAttention.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    250         relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n    251         attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n--> 253 attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n    254 if attention_mask is not None:\n    255     # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n    256     attention_scores = attention_scores + attention_mask\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 88.00 MiB. GPU \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Start the timer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m pm\u001b[39m.\u001b[39;49mexecute_notebook(\n\u001b[1;32m     21\u001b[0m     input_nb,\n\u001b[1;32m     22\u001b[0m     output_nb,\n\u001b[1;32m     23\u001b[0m     parameters\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mnoise\u001b[39;49m\u001b[39m'\u001b[39;49m: noise, \n\u001b[1;32m     24\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m'\u001b[39;49m: seed, \n\u001b[1;32m     25\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mversion\u001b[39;49m\u001b[39m'\u001b[39;49m: version,\n\u001b[1;32m     26\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39msample_size\u001b[39;49m\u001b[39m'\u001b[39;49m: sample_size}\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[39m# Stop the timer\u001b[39;00m\n\u001b[1;32m     30\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/papermill/execute.py:131\u001b[0m, in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, language, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m         nb \u001b[39m=\u001b[39m papermill_engines\u001b[39m.\u001b[39mexecute_notebook_with_engine(\n\u001b[1;32m    117\u001b[0m             engine_name,\n\u001b[1;32m    118\u001b[0m             nb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mengine_kwargs,\n\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    130\u001b[0m     \u001b[39m# Check for errors first (it saves on error before raising)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     raise_for_execution_errors(nb, output_path)\n\u001b[1;32m    133\u001b[0m \u001b[39m# Write final output in case the engine didn't write it on cell completion.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m write_ipynb(nb, output_path)\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/papermill/execute.py:251\u001b[0m, in \u001b[0;36mraise_for_execution_errors\u001b[0;34m(nb, output_path)\u001b[0m\n\u001b[1;32m    248\u001b[0m nb\u001b[39m.\u001b[39mcells\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, error_msg_cell)\n\u001b[1;32m    250\u001b[0m write_ipynb(nb, output_path)\n\u001b[0;32m--> 251\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mPapermillExecutionError\u001b[0m: \n---------------------------------------------------------------------------\nException encountered at \"In [20]\":\n---------------------------------------------------------------------------\nOutOfMemoryError                          Traceback (most recent call last)\nCell In[20], line 12\n      8 learning_rate_history = []\n     10 for epoch in pbar:\n---> 12     train_loss, metrics, current_lr = yuri.train(train_loader)\n     14     training_losses.append(train_loss)\n     15     metric_history.append(metrics)\n\nFile ~/llmft/llmft/train.py:66, in EncoderTrainer.train(self, data_loader)\n     63 all_labels = []\n     65 for batch in data_loader:\n---> 66     loss, predictions, labels = self.process_batch(batch, train=True)\n     67     total_loss += loss.item()\n     68     all_predictions.append(predictions)\n\nFile ~/llmft/llmft/train.py:89, in EncoderTrainer.process_batch(self, batch, train)\n     85 \"\"\"Process a single batch of data.\"\"\"\n     86 input_ids, attention_mask, labels = (batch['input_ids'].to(self.device), \n     87                                      batch['attention_mask'].to(self.device), \n     88                                      batch['labels'].to(self.device))\n---> 89 logits = self.model(input_ids, attention_mask).logits\n     90 if self.criterion.mode == 'input' and 'type_indicator' in batch:\n     91     type_indicator = batch['type_indicator'].to(self.device)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1191, in RobertaForSequenceClassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n   1183 r\"\"\"\n   1184 labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n   1185     Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n   1186     config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n   1187     `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n   1188 \"\"\"\n   1189 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-> 1191 outputs = self.roberta(\n   1192     input_ids,\n   1193     attention_mask=attention_mask,\n   1194     token_type_ids=token_type_ids,\n   1195     position_ids=position_ids,\n   1196     head_mask=head_mask,\n   1197     inputs_embeds=inputs_embeds,\n   1198     output_attentions=output_attentions,\n   1199     output_hidden_states=output_hidden_states,\n   1200     return_dict=return_dict,\n   1201 )\n   1202 sequence_output = outputs[0]\n   1203 logits = self.classifier(sequence_output)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:828, in RobertaModel.forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    819 head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n    821 embedding_output = self.embeddings(\n    822     input_ids=input_ids,\n    823     position_ids=position_ids,\n   (...)\n    826     past_key_values_length=past_key_values_length,\n    827 )\n--> 828 encoder_outputs = self.encoder(\n    829     embedding_output,\n    830     attention_mask=extended_attention_mask,\n    831     head_mask=head_mask,\n    832     encoder_hidden_states=encoder_hidden_states,\n    833     encoder_attention_mask=encoder_extended_attention_mask,\n    834     past_key_values=past_key_values,\n    835     use_cache=use_cache,\n    836     output_attentions=output_attentions,\n    837     output_hidden_states=output_hidden_states,\n    838     return_dict=return_dict,\n    839 )\n    840 sequence_output = encoder_outputs[0]\n    841 pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:517, in RobertaEncoder.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\n    506     layer_outputs = self._gradient_checkpointing_func(\n    507         layer_module.__call__,\n    508         hidden_states,\n   (...)\n    514         output_attentions,\n    515     )\n    516 else:\n--> 517     layer_outputs = layer_module(\n    518         hidden_states,\n    519         attention_mask,\n    520         layer_head_mask,\n    521         encoder_hidden_states,\n    522         encoder_attention_mask,\n    523         past_key_value,\n    524         output_attentions,\n    525     )\n    527 hidden_states = layer_outputs[0]\n    528 if use_cache:\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:406, in RobertaLayer.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    394 def forward(\n    395     self,\n    396     hidden_states: torch.Tensor,\n   (...)\n    403 ) -> Tuple[torch.Tensor]:\n    404     # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n    405     self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n--> 406     self_attention_outputs = self.attention(\n    407         hidden_states,\n    408         attention_mask,\n    409         head_mask,\n    410         output_attentions=output_attentions,\n    411         past_key_value=self_attn_past_key_value,\n    412     )\n    413     attention_output = self_attention_outputs[0]\n    415     # if decoder, the last output is tuple of self-attn cache\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:333, in RobertaAttention.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    323 def forward(\n    324     self,\n    325     hidden_states: torch.Tensor,\n   (...)\n    331     output_attentions: Optional[bool] = False,\n    332 ) -> Tuple[torch.Tensor]:\n--> 333     self_outputs = self.self(\n    334         hidden_states,\n    335         attention_mask,\n    336         head_mask,\n    337         encoder_hidden_states,\n    338         encoder_attention_mask,\n    339         past_key_value,\n    340         output_attentions,\n    341     )\n    342     attention_output = self.output(self_outputs[0], hidden_states)\n    343     outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-> 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/llmft/llms/lib/python3.10/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-> 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/llmft/llms/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:253, in RobertaSelfAttention.forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\n    250         relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n    251         attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n--> 253 attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n    254 if attention_mask is not None:\n    255     # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n    256     attention_scores = attention_scores + attention_mask\n\nOutOfMemoryError: CUDA out of memory. Tried to allocate 88.00 MiB. GPU \n"
     ]
    }
   ],
   "source": [
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] \n",
    "\n",
    "noises = [True]\n",
    "version = 4\n",
    "sample_size = 1000\n",
    "input_nb = './single_run.ipynb'\n",
    "\n",
    "# List to store execution times\n",
    "execution_times = []\n",
    "\n",
    "for noise in noises: \n",
    "\n",
    "    print(f'Starting Experiment --> verion: {version}  |   sample size: {sample_size}  |   noise: {noise}')\n",
    "    for i, seed in enumerate(seeds):\n",
    "        output_nb = f'output_notebook_{i+1}.ipynb'\n",
    "        \n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        pm.execute_notebook(\n",
    "            input_nb,\n",
    "            output_nb,\n",
    "            parameters={'noise': noise, \n",
    "                        'seed': seed, \n",
    "                        'version': version,\n",
    "                        'sample_size': sample_size}\n",
    "        )\n",
    "        \n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the duration\n",
    "        duration = end_time - start_time\n",
    "        execution_times.append(duration)\n",
    "        \n",
    "        print(f\"Run {i+1} with seed {seed} took {duration:.2f} seconds\")\n",
    "\n",
    "        #Remove the output notebook after capturing necessary data\n",
    "        os.remove(output_nb)\n",
    "\n",
    "    # Optionally, you can save these times to a file or print them all out at the end\n",
    "    print(\"All execution times:\", execution_times)\n",
    "    print(f'Ending Experiment with {noise}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
