{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import random \n",
    "import math\n",
    "from itertools import chain\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "import tiktoken\n",
    "import csv\n",
    "import time \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "from trics.nlp.utils import to_markdown, create_csv_with_headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = 1 \n",
    "experiment_folder = './../../../toy-data/exp2/'\n",
    "data_csv = experiment_folder + f'data_{version}.csv'\n",
    "df = pd.read_csv(data_csv)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../../../styles/Newsreader\n",
      "/home/ubuntu/llmft/styles/Newsreader/static/Newsreader_14pt/Newsreader_14pt-BoldItalic.ttf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['image.interpolation'] = 'nearest'\n",
    "rcParams['image.cmap'] = 'viridis'\n",
    "rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "\n",
    "from matplotlib import font_manager \n",
    "locations = './../../../styles/Newsreader'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "print(locations)\n",
    "print(font_files[0])\n",
    "for f in font_files: \n",
    "    font_manager.fontManager.addfont(f)\n",
    "plt.rcParams[\"font.family\"] = \"Newsreader\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLM Set Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 1 already exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(data_csv):\n",
    "    print(f'Data for {version} already exists')\n",
    "    run = False \n",
    "else:\n",
    "    create_csv_with_headers(data_csv, ['Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'Var9', 'Description'])\n",
    "    run = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = [\"mid-20s\", \"late-20s\", \"early-30s\", \"mid-30s\", \"late-30s\", \"early-40s\", \"mid-40s\", \"late-40s\", \"early-50s\", \"mid-50s\", \"late-50s\"]\n",
    "living_situations = [\"small apartment complex\", \"large apartment complex\", \"three-story duplex\", \"two-story duplex\"]\n",
    "pets_options = [\"a dog\", \"a cat\", \"no pets\", \"a bird\", \"a fish tank\"]\n",
    "overdue_phrases = [\"behind on rent for\", \"owing rent amounting to\", \"late on rent for\", \"struggling to pay rent for\"]\n",
    "additional_details = [\n",
    "        \"the tenant enjoys quiet evenings at home.\",\n",
    "        \"the tenant frequently interacts with neighbors.\",\n",
    "        \"the tenant has a regular job in the city.\",\n",
    "        \"the tenant occasionally works from home.\",\n",
    "        \"the tenant likes to cook on weekends.\",\n",
    "        \"the tenant has a small garden on the balcony.\",\n",
    "        \"the tenant is an avid reader and has a collection of books.\",\n",
    "        \"the tenant enjoys jogging in the nearby park.\",\n",
    "        \"the tenant has recently taken up painting as a hobby.\",\n",
    "        \"the tenant volunteers at a local shelter on weekends.\",\n",
    "        \"the tenant loves to bake and often shares treats with neighbors.\",\n",
    "        \"the tenant has a passion for photography and takes pictures around the city.\",\n",
    "        \"the tenant plays the guitar in their spare time.\",\n",
    "        \"the tenant participates in a local book club.\",\n",
    "        \"the tenant practices yoga every morning.\",\n",
    "        \"the tenant is learning to play the piano.\",\n",
    "        \"the tenant loves to travel and plans trips frequently.\",\n",
    "        \"the tenant enjoys watching movies on weekends.\",\n",
    "        \"the tenant is a member of a local sports team.\",\n",
    "        \"the tenant loves to host dinner parties for friends.\",\n",
    "        \"the tenant is studying for an advanced degree online.\",\n",
    "        \"the tenant enjoys gardening in a community garden.\",\n",
    "        \"the tenant is a fan of board games and has a collection at home.\",\n",
    "        \"the tenant frequently visits local museums.\",\n",
    "        \"the tenant enjoys knitting and crafts in their spare time.\",\n",
    "        \"the tenant has a side business selling handmade items.\",\n",
    "        \"the tenant enjoys hiking and exploring nature trails.\",\n",
    "        \"the tenant volunteers at a local animal shelter.\",\n",
    "        \"the tenant is a coffee enthusiast and loves visiting new cafes.\",\n",
    "        \"the tenant enjoys going to live music concerts.\",\n",
    "        \"the tenant participates in local charity events.\",\n",
    "        \"the tenant loves to explore different cuisines and tries new recipes.\",\n",
    "        \"the tenant has a keen interest in astronomy and stargazing.\",\n",
    "        \"the tenant enjoys writing short stories in their free time.\",\n",
    "        \"the tenant is learning a new language.\",\n",
    "        \"the tenant loves attending theater performances.\",\n",
    "        \"the tenant is part of a local dance class.\",\n",
    "        \"the tenant enjoys playing video games.\",\n",
    "        \"the tenant is a foodie and enjoys dining out at new restaurants.\",\n",
    "        \"the tenant likes to go fishing on weekends.\",\n",
    "        \"the tenant enjoys cycling around the city.\",\n",
    "        \"the tenant has a small art studio at home.\",\n",
    "        \"the tenant likes to play chess with friends.\",\n",
    "        \"the tenant participates in local marathons.\",\n",
    "        \"the tenant enjoys taking part in cooking classes.\",\n",
    "        \"the tenant loves to decorate their apartment for different holidays.\",\n",
    "        \"the tenant frequently attends local farmers' markets.\",\n",
    "        \"the tenant enjoys exploring local history and landmarks.\",\n",
    "        \"the tenant has a blog where they share their daily experiences.\"\n",
    "    ]\n",
    "roomate_statuses = ['roomate', 'family']\n",
    "contribute_statuses = [\"doesn't contribute\", \"contributes\"]   \n",
    "disabilities = [\"good health\", \n",
    "                \"mobility impairments\",\n",
    "                \"blindness\", \n",
    "                \"deafness\", \n",
    "                \"multiple sclerosis\",\n",
    "                \"cerebral palsy.\",\n",
    "                \"severe depression\", \n",
    "                \"bipolar disorder\", \n",
    "                \"schizophrenia\",\n",
    "                \"anxiety disorders\",\n",
    "                \"autism\",\n",
    "                \"diabetes\", \n",
    "                \"heart disease\", \n",
    "                \"HIV/AIDS.\",\n",
    "                \"a major surgery\",\n",
    "                \"a severe injury\",\n",
    "                \"pneumonia\",\n",
    "                \"Post-Traumatic Stress\",\n",
    "                \"rehabing from addiction\"]\n",
    "voucher_status = ['with a housing voucher', '']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_completion(input):\n",
    "    message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    temperature=1.,\n",
    "    system=\"You are a housing court lawyer\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": input}\n",
    "    ])\n",
    "    return message.content[0].text\n",
    "\n",
    "\n",
    "def get_promptv3(i, x):\n",
    "    \"\"\"Generates a prompt for writing a paragraph about a tenant with added noise for variability.\n",
    "\n",
    "    Args:\n",
    "        i: Random seed for reproducibility.\n",
    "        x: List of features for the tenant.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string containing the prompt.\n",
    "    \"\"\"\n",
    "    random.seed(i)\n",
    "\n",
    "    # Define possible synonyms and additional details\n",
    "\n",
    "    # Add variability\n",
    "    age_group = age_groups[x[0]]\n",
    "    living_situation = living_situations[x[1]]\n",
    "    pets = pets_options[x[2]]\n",
    "    rent = x[3]\n",
    "    health = disabilities[x[4]]\n",
    "    months = x[5]\n",
    "    roomate_status = roomate_statuses[x[6]]\n",
    "    contribute_status = contribute_statuses[x[7]]\n",
    "    voucher = voucher_status[x[8]]\n",
    "\n",
    "    # Add some noise with random synonyms or additional details\n",
    "    overdue_phrase = random.choice(overdue_phrases)\n",
    "    additional_detail = random.choice(additional_details)\n",
    "\n",
    "    return f\"\"\"random seed: {i}\n",
    "    Task: Write a paragraph description of a tenant in their {age_group} who is currently {overdue_phrase} ${rent:.0f}. \n",
    "    Mention that they have {health}, live in a {living_situation} {voucher}, have been living there for {months} months, and have {pets}. \n",
    "    Include some details about their {roomate_status} who {contribute_status} to the rent. Also mention somewhere that {additional_detail}\n",
    "    \n",
    "    Description: The tenant is a \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 9)\n"
     ]
    }
   ],
   "source": [
    "# Set number of observations\n",
    "n = 25_000 \n",
    "\n",
    "# Number of elements in the list\n",
    "n_disabilities = len(disabilities)\n",
    "\n",
    "# Define the probabilities\n",
    "probabilities = [0.60] + [(1 - 0.60) / (n_disabilities - 1)] * (n_disabilities - 1)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(2)\n",
    "\n",
    "# Generate a single Bernoulli random variable with p = 0.5\n",
    "var1 = np.random.choice(range(len(age_groups)), size=n).astype(int)\n",
    "var2 = np.random.choice(range(len(living_situations)), size=n).astype(int)\n",
    "var3 = np.random.choice(range(len(pets_options)), size=n).astype(int)\n",
    "var4 = np.random.choice(range(650, 1500), size=n).astype(int)\n",
    "var5 = np.random.choice(range(n_disabilities), size=n, p=probabilities)\n",
    "var6 = np.random.choice(range(6, 54), size=n).astype(int)\n",
    "var7 = np.random.choice(range(2), size=n).astype(int)\n",
    "var8 = np.random.choice(range(2), size=n).astype(int)\n",
    "var9 = np.random.binomial(n=1, p=0.15, size=n).astype(int)\n",
    "data_matrix = np.column_stack((var1, var2, var3, var4, var5, var6, var7, var8, var9))\n",
    "print(data_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/17005 [01:10<10:07:08,  2.15s/it]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m7995\u001b[39m, n)):\n\u001b[0;32m----> 2\u001b[0m     text \u001b[39m=\u001b[39m anthropic_completion(get_promptv3(i, data_matrix[i]))\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_csv, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m         writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(file)\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36manthropic_completion\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manthropic_completion\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     message \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mmessages\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      3\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclaude-3-haiku-20240307\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m1.\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     system\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mYou are a housing court lawyer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m      8\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39minput\u001b[39;49m}\n\u001b[1;32m      9\u001b[0m     ])\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m message\u001b[39m.\u001b[39mcontent[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/resources/messages.py:899\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    867\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    868\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m \u001b[39m600\u001b[39m,\n\u001b[1;32m    898\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Message \u001b[39m|\u001b[39m Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    900\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/v1/messages\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    901\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    902\u001b[0m             {\n\u001b[1;32m    903\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    904\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    905\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    906\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[1;32m    907\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop_sequences\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop_sequences,\n\u001b[1;32m    908\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    909\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m: system,\n\u001b[1;32m    910\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    911\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    912\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    913\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_k,\n\u001b[1;32m    914\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    915\u001b[0m             },\n\u001b[1;32m    916\u001b[0m             message_create_params\u001b[39m.\u001b[39;49mMessageCreateParams,\n\u001b[1;32m    917\u001b[0m         ),\n\u001b[1;32m    918\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    919\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    920\u001b[0m         ),\n\u001b[1;32m    921\u001b[0m         cast_to\u001b[39m=\u001b[39;49mMessage,\n\u001b[1;32m    922\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    923\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[RawMessageStreamEvent],\n\u001b[1;32m    924\u001b[0m     )\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1226\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1227\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1235\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1236\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1237\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1239\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1004\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1003\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1005\u001b[0m         options,\n\u001b[1;32m   1006\u001b[0m         cast_to,\n\u001b[1;32m   1007\u001b[0m         retries,\n\u001b[1;32m   1008\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1009\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1010\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1052\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1052\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1053\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1054\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1055\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1056\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1057\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1058\u001b[0m )\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1004\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1003\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1005\u001b[0m         options,\n\u001b[1;32m   1006\u001b[0m         cast_to,\n\u001b[1;32m   1007\u001b[0m         retries,\n\u001b[1;32m   1008\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1009\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1010\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1052\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1052\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1053\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1054\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1055\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1056\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1057\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1058\u001b[0m )\n",
      "File \u001b[0;32m~/llmft/llms/lib/python3.10/site-packages/anthropic/_base_client.py:1019\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1018\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1022\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1023\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(7995, n)):\n",
    "    text = anthropic_completion(get_promptv3(i, data_matrix[i]))\n",
    "    with open(data_csv, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        row = list(data_matrix[i]) + [text]\n",
    "        writer.writerow(row)\n",
    "    time.sleep(0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
