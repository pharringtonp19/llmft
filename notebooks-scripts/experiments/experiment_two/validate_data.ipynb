{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# Text processing libraries\n",
    "import tiktoken\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Utility libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "\n",
    "# Custom libraries  \n",
    "from llmft.train import EncoderTrainer, EarlyStopping\n",
    "from llmft.metrics import compute_recall, compute_f1_score\n",
    "from llmft.losses import FocalLoss\n",
    "from llmft.utils import predict\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns  # Assuming seaborn is installed\n",
    "\n",
    "# NLP utility (assuming trics is a library/module)\n",
    "from trics.nlp.utils import to_markdown\n",
    "\n",
    "# Configure GPU usage and tokenizer parallelism\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Dataset libraries (can be grouped together)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 1                        # Seed\n",
    "prompt = True                  # Whether to include a prompt about who is likely to take up the treatment if offered\n",
    "noise = False                    # Whether covariates can predict the takeup of treatment\n",
    "sample_size = 10_000              # Sample Size\n",
    "val_set_fraction = 0.25         # Fraction of sample used for validation set\n",
    "data_version = 1                # version of synthetic data         \n",
    "class_weight_type = 'standard'  \n",
    "lr = 2e-4                       # Optimizer learning rate\n",
    "warmup_ratio = 0.25             # Fraction of training epochs used for learning rate warm up\n",
    "batch_size = 32                 # Number of observations in each mini-batch\n",
    "epochs = 50                     # Number of training epochs\n",
    "patience = 30                    \n",
    "gamma = 0.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Up Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = f'./../../../toy-data/exp2/data_{data_version}.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Up Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../../../styles/Newsreader\n",
      "/home/ubuntu/llmft/styles/Newsreader/static/Newsreader_24pt/Newsreader_24pt-MediumItalic.ttf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['image.interpolation'] = 'nearest'\n",
    "rcParams['image.cmap'] = 'viridis'\n",
    "rcParams['axes.grid'] = False\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "\n",
    "from matplotlib import font_manager \n",
    "locations = './../../../styles/Newsreader'\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "print(locations)\n",
    "print(font_files[0])\n",
    "for f in font_files: \n",
    "    font_manager.fontManager.addfont(f)\n",
    "plt.rcParams[\"font.family\"] = \"Newsreader\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Stage Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_not_covered = [\n",
    "    \"Short-term illnesses\",\n",
    "    \"Temporary exhaustion\",\n",
    "    \"Behavioral Traits and Conditions\",\n",
    "    \"Having a quick temper without any underlying medical condition\",\n",
    "    \"Certain Addictions\",\n",
    "    \"Substance abuse disorders\",\n",
    "    \"good health\"\n",
    "]\n",
    "\n",
    "conditions_covered = [\n",
    "    \"Mobility impairments\",\n",
    "    \"Visual impairments\",\n",
    "    \"Hearing impairments\",\n",
    "    \"Chronic illnesses\",\n",
    "    \"Respiratory disorders\",\n",
    "    \"Cardiovascular conditions\",\n",
    "    \"Intellectual disabilities\",\n",
    "    \"Learning disabilities\",\n",
    "    \"Autism spectrum disorders\",\n",
    "    \"Psychiatric disorders\",\n",
    "    \"Traumatic brain injuries\",\n",
    "    \"Alzheimer's disease and other dementias\"\n",
    "]\n",
    "\n",
    "disabilities = conditions_covered + conditions_not_covered\n",
    "\n",
    "# Create a list of (0,1)s\n",
    "severity_indicator = [1 if disability in conditions_covered else 0 for disability in disabilities]\n",
    "\n",
    "def fstage(var1, var2, var3, var4, var5, var6, var7, var8, var9):\n",
    "    return 1*var9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_information(text):\n",
    "    # Define simple search terms\n",
    "    has_lawyer_access = 'tenant has access to a free lawyer' in text\n",
    "    has_voucher = 'housing voucher' in text\n",
    "    \n",
    "    # Define conditions\n",
    "    conditions = [\n",
    "        \"Short-term illnesses\",\n",
    "        \"Temporary exhaustion\",\n",
    "        \"Behavioral Traits and Conditions\",\n",
    "        \"Having a quick temper without any underlying medical condition\",\n",
    "        \"Certain Addictions\",\n",
    "        \"Substance abuse disorders\",\n",
    "        \"good health\",\n",
    "        \"Mobility impairments\",\n",
    "        \"Visual impairments\",\n",
    "        \"Hearing impairments\",\n",
    "        \"Chronic illnesses\",\n",
    "        \"Respiratory disorders\",\n",
    "        \"Cardiovascular conditions\",\n",
    "        \"Intellectual disabilities\",\n",
    "        \"Learning disabilities\",\n",
    "        \"Autism spectrum disorders\",\n",
    "        \"Psychiatric disorders\",\n",
    "        \"Traumatic brain injuries\",\n",
    "        \"Alzheimer's disease and other dementias\"\n",
    "    ]\n",
    "    \n",
    "    # Check for conditions\n",
    "    condition_status = [condition for condition in conditions if condition.lower() in text.lower()]\n",
    "    condition_status = f\"covered conditions: {', '.join(condition_status)}\"\n",
    "    lawyer_status = \"tenant has access to legal representation\" if has_lawyer_access else \"the right to counsel is not in effect\"\n",
    "    voucher_status = \"tenant has a housing voucher\" if has_voucher else \"tenant does not have a housing voucher\"\n",
    "    result = f\"{lawyer_status}, {voucher_status}, {condition_status}\"\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Read in Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\n+', '', text)\n",
    "\n",
    "string_b = 'Based on the following information, predict (Yes/No) whether the tenant will be represented in court by a lawyer.\\n\\n'\n",
    "string1 = \"Note: Some tenants have access to a free lawyer others don't. This tenant **has access** to a free lawyer.\\n\\n\"\n",
    "string0 = \"Note: Some tenants have access to a free lawyer others don't. This tenant **does not have access** to a free lawyer.\\n\\n\"\n",
    "if prompt:\n",
    "\n",
    "    context = \"Also Note: Access to a lawyer does not mean the tenant will be represented in court by a lawyer.\" \\\n",
    "        \" If the tenant is given access to a lawyer, they must apply for representation.\" \\\n",
    "        \" Because more tenants apply than can be represented, legal aid providers prioritize tenants with **vouchers** and **disabilities** when reviewing applications.\" \\\n",
    "        \" Providers can differ over what they consider to be a dissability.\"\\\n",
    "        \" If a tenant's application is selected, they must follow-up with the provider to arange for legal representation.\" \\\n",
    "        \" Therefor it's possible that tenants with characteristics which sugges they they should be prioritized remain without representation.\"\n",
    "else:\n",
    "    context = ''\n",
    "\n",
    "# Read in Data Set\n",
    "df = pd.read_csv(data_csv)\n",
    "df['Description'] = df['Description'].apply(lambda x: remove_newlines(x))\n",
    "\n",
    "\n",
    "# Subsample Observations\n",
    "indices = np.random.choice(df.index, size=sample_size, replace=False)\n",
    "df = df.loc[indices].reset_index(drop=True)\n",
    "\n",
    "# Apply First Stage Function\n",
    "df['FStage_Value'] = df.apply(lambda row: fstage(\n",
    "                                                 row['Var1'], \n",
    "                                                 row['Var2'], \n",
    "                                                 row['Var3'], \n",
    "                                                 row['Var4'], \n",
    "                                                 row['Var5'],\n",
    "                                                 row['Var6'],\n",
    "                                                 row['Var7'],\n",
    "                                                 row['Var8'],\n",
    "                                                 row['Var9']), axis=1)\n",
    "\n",
    "# If Noise: Shuffle the first stage values\n",
    "if noise: \n",
    "    df['FStage_Value'] = df['FStage_Value'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Sample Instrumental Values  \n",
    "df['Instrument'] = np.random.binomial(n=1, p=0.5, size=sample_size)\n",
    "\n",
    "# Text + Instrument\n",
    "df['FullDescription'] = np.where(df['Instrument'] == 1,\n",
    "                             string_b + string1 + 'Description:' + df['Description'].replace(\"\\n\", \"\") + '\\n\\n' + context,\n",
    "                             string_b + string0 + 'Description:' + df['Description'].replace(\"\\n\", \"\") + '\\n\\n'+ context)\n",
    "\n",
    "# Text + Instrument == 1\n",
    "df['Treated_FullDescription'] = df.apply(lambda row: string_b + string1 + 'Description:' + row['Description'].replace(\"\\n\", \"\") + '\\n\\n' + context, axis=1)\n",
    "\n",
    "# Text + Instrument == 0\n",
    "df['Control_FullDescription'] = df.apply(lambda row: string_b + string0 + 'Description:' + row['Description'].replace(\"\\n\", \"\") + '\\n\\n' + context, axis=1)\n",
    "\n",
    "# Sample Treatment Values\n",
    "df['Treatment'] = np.random.binomial(n=1, p= df['FStage_Value'] * df['Instrument'], size=sample_size)\n",
    "\n",
    "# Sample Outcome Values\n",
    "df['Outcome'] =  (1. +  2.*(df['FStage_Value'] > 0.))*df['Treatment'] + 0.1*np.random.normal(size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary\n",
    "def build_vocab(sentences):\n",
    "    vocab = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            vocab[word] += 1\n",
    "    return {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "\n",
    "vocab = build_vocab(df.FullDescription.apply(extract_information))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0766"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = (df.FullDescription.apply(extract_information).str.contains('the right to counsel', case=False, na=False)) & \n",
    "df[ ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding sentences as BoW vectors\n",
    "def encode_sentence(sentence, vocab):\n",
    "    vector = torch.zeros(len(vocab))\n",
    "    for word in sentence.split():\n",
    "        if word in vocab:\n",
    "            vector[vocab[word]] += 1\n",
    "    return vector\n",
    "\n",
    "# Encode all sentences\n",
    "X = torch.stack([encode_sentence(sentence, vocab) for sentence in df.FullDescription.apply(extract_information)])\n",
    "y = torch.tensor(df.Treatment, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BoWModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.optim' has no attribute 'sgd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m BoWModel(vocab_size)\n\u001b[1;32m      3\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49msgd(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Training the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.optim' has no attribute 'sgd'"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "model = BoWModel(vocab_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X).squeeze()\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "        sns.kdeplot(outputs.detach().numpy(), color='blue', fill=False, bw_adjust=0.25, label='Train')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(X).squeeze()\n",
    "outputs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contains_housing_voucher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows with 'housing voucher'\n",
    "housing_voucher_df = df[(df['contains_housing_voucher']) & (df['Instrument'])]\n",
    "\n",
    "# Compute the fraction of those with 'housing voucher' who receive treatment\n",
    "fraction_with_treatment = housing_voucher_df['Treatment'].mean()\n",
    "\n",
    "print(f\"Fraction of those with a housing voucher who receive the treatment: {fraction_with_treatment:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
